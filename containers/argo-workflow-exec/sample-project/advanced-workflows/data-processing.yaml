apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: data-processing-
  annotations:
    description: "ETL data processing pipeline workflow example"
spec:
  entrypoint: data-pipeline
  arguments:
    parameters:
    - name: source-database
      value: "production-db"
    - name: target-database
      value: "analytics-db"
  templates:
  - name: data-pipeline
    dag:
      tasks:
      - name: extract
        template: extract-data
      - name: transform
        template: transform-data
        dependencies: [extract]
      - name: load
        template: load-data
        dependencies: [transform]
      - name: validate
        template: validate-data
        dependencies: [load]
  - name: extract-data
    container:
      image: python:3.9-slim
      command: [python, -c]
      args: |
        import json
        import time
        
        print("ğŸ”„ Starting data extraction...")
        print(f"Source database: {{workflow.parameters.source-database}}")
        
        # Simulate data extraction
        data = {
          "source": "{{workflow.parameters.source-database}}",
          "records": 1000,
          "timestamp": time.time(),
          "status": "extracted"
        }
        
        with open("/tmp/extracted_data.json", "w") as f:
          json.dump(data, f)
        
        print(f"âœ… Extracted {data['records']} records")
        print("ğŸ“ Data saved to /tmp/extracted_data.json")
      resources:
        requests:
          memory: "128Mi"
          cpu: "200m"
        limits:
          memory: "256Mi"
          cpu: "500m"
  - name: transform-data
    container:
      image: python:3.9-slim
      command: [python, -c]
      args: |
        import json
        import time
        
        print("ğŸ”„ Starting data transformation...")
        
        # Read extracted data
        with open("/tmp/extracted_data.json", "r") as f:
          data = json.load(f)
        
        # Transform data
        data["transformed"] = True
        data["processed_records"] = data["records"] * 2
        data["transformation_timestamp"] = time.time()
        data["quality_score"] = 0.95
        
        # Add some computed fields
        data["computed_fields"] = {
          "avg_value": 42.5,
          "total_sum": 42500,
          "record_count": data["processed_records"]
        }
        
        with open("/tmp/transformed_data.json", "w") as f:
          json.dump(data, f)
        
        print(f"âœ… Transformed {data['processed_records']} records")
        print(f"ğŸ“Š Quality score: {data['quality_score']}")
        print("ğŸ“ Transformed data saved to /tmp/transformed_data.json")
      resources:
        requests:
          memory: "128Mi"
          cpu: "200m"
        limits:
          memory: "256Mi"
          cpu: "500m"
  - name: load-data
    container:
      image: python:3.9-slim
      command: [python, -c]
      args: |
        import json
        import time
        
        print("ğŸ”„ Starting data loading...")
        print(f"Target database: {{workflow.parameters.target-database}}")
        
        # Read transformed data
        with open("/tmp/transformed_data.json", "r") as f:
          data = json.load(f)
        
        # Simulate loading to target database
        print(f"ğŸ“¥ Loading {data['processed_records']} records to {{workflow.parameters.target-database}}")
        time.sleep(2)  # Simulate loading time
        
        data["loaded"] = True
        data["load_timestamp"] = time.time()
        data["target_database"] = "{{workflow.parameters.target-database}}"
        
        with open("/tmp/loaded_data.json", "w") as f:
          json.dump(data, f)
        
        print("âœ… Data loading completed successfully!")
        print("ğŸ“ Load summary saved to /tmp/loaded_data.json")
      resources:
        requests:
          memory: "128Mi"
          cpu: "200m"
        limits:
          memory: "256Mi"
          cpu: "500m"
  - name: validate-data
    container:
      image: python:3.9-slim
      command: [python, -c]
      args: |
        import json
        
        print("ğŸ” Starting data validation...")
        
        # Read loaded data
        with open("/tmp/loaded_data.json", "r") as f:
          data = json.load(f)
        
        # Perform validation checks
        validation_results = {
          "total_records": data["processed_records"],
          "quality_score": data["quality_score"],
          "completeness": 0.98,
          "accuracy": 0.97,
          "consistency": 0.99,
          "overall_score": 0.96
        }
        
        print("ğŸ“Š Validation Results:")
        for key, value in validation_results.items():
          print(f"  {key}: {value}")
        
        if validation_results["overall_score"] >= 0.95:
          print("âœ… Data validation passed!")
          print("ğŸ‰ ETL pipeline completed successfully!")
        else:
          print("âŒ Data validation failed!")
          raise Exception("Data quality below threshold")
        
        # Save validation report
        with open("/tmp/validation_report.json", "w") as f:
          json.dump(validation_results, f)
      resources:
        requests:
          memory: "64Mi"
          cpu: "100m"
        limits:
          memory: "128Mi"
          cpu: "200m"
